
\ijcv{
\subsection{\gcam{} is class-discriminative}
In this section we show why \gcam{} visualizations are class-discriminative.
Recall that $\alpha^k_c$~\eqref{eq:alpha1} can be extracted from any layer of the deep CNN.
Consider a simple cascaded deep CNN for classification having non-linear activation functions given by $\sigma_l(.)$ between layers $l$ and $(l+1)$. The scores corresponding to classes can be expressed as $\mathbf{y} = W_f^TA_f$ where $W_f$ and $A_f$ correspond to the final layer weights and activations respectively. 
Let $o_c$ be the incoming gradient from the loss layer. Note that $o_c$ is a one-hot vector with $1$ at the dimension corresponding to class $c$ and $0$ everywhere else.
With this, $\alpha_c^k$ of \gcam{} can be written in terms of $\mathbf{y}$ and $o_c$ as,
\begin{ceqn}
\begin{equation}
\alpha_c^k = \frac{1}{Z} \sum_i \sum_j \frac{\partial (\mathbf{y}\circ o_c)}{\partial A^k_{ij}}
\end{equation}
\end{ceqn}
where $\circ$ denotes the elementwise product.
Since $o_c$ is a constant, $\frac{\partial (o_c)}{\partial A^k_{ij}} = 0$. Thus we obtain,
\begin{ceqn}
\begin{equation}
\alpha_c^k = (\frac{1}{Z} \sum_i \sum_j \frac{\partial \mathbf{y}}{\partial A^k_{ij}})\circ o_c
\end{equation}
\end{ceqn}
Now, recall that due to chain rule, this gradient expression above can be rewritten as a product of partial derivatives along the computation path:
\begin{ceqn}
\begin{equation}
 \alpha_c^k = (\frac{1}{Z} \sum_{i,j} \frac{\partial \mathbf{y}}{\partial A_{f-1}} \frac{\partial A_{f-1}}{\partial \sigma_{A_{f-1}}} \frac{\partial \sigma_{A_{f-1}}}{\partial A_{f-2}}\frac{\partial A_{f-2}}{\partial \sigma_{A_{f-2}}}...)\circ o_c
\end{equation}
\end{ceqn}
Recall that $\frac{\partial (y)}{\partial A_{f-1}}$ is $W_f$, and similarly $\frac{\partial \sigma_{A_{l}}}{\partial A_{l-1}}$ is $W_{l}$.
The gradients \wrt activation functions results in diagonal matrices, $D_{\sigma_l}$. For networks with ReLU activations, the entries in the diagonal matrices are either 1 or 0 depending on whether the forward activations at that location is positive or negative, respectively. By substituting we get,
\begin{ceqn}
\begin{equation}
 \alpha_c^k = (\frac{1}{Z} \sum_{i,j} W_f D_{\sigma_{f-1}} W_{f-1} D_{\sigma_{f-2}}...)\circ o_c
\end{equation}
\end{ceqn}
The above expression can be reduced to,
\begin{ceqn}
\begin{equation}
 \alpha_c^k = (\frac{1}{Z} \sum_{i,j} \prod_{l=f}^L D_{\sigma_{l-1}} W_{l}) \circ o_c
\end{equation}
\end{ceqn}
where $W_{l}$ denote the weights connecting layers $(l-1)$ and $l$, and $l = \{ f, f-1, ..., L \}$.
Therefore, $\alpha_c^k$ explicitly captures the dynamics of the pathways leading from $L$ (last convolutional layer) to the target class score, $y_c$ in the network.
This makes neuron-importance weights, $\alpha_c^k$ and by extension \gcam{} class-discriminative.}










