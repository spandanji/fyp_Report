\section*{Appendix I: \gcam{} as generalization of CAM}\label{sec:sup_generalization}

In this section we formally prove that \gcam{} is a generalization of CAM, as mentioned in Section 3 in the main paper.

Recall that the CAM architecture consists of fully-covolutional CNNs, followed by global average pooling, and linear classification layer with softmax.

Let the final convolutional layer produce $K$ feature maps $A^k$, with each element indexed by $i,j$. 
So $A_{ij}^k$ refers to the activation at location $(i,j)$ of the feature map $A^k$.

CAM computes a global average pooling (GAP) on $A_{ij}^k$. Let us define $F^k$ to be the global average pooled output,

So,
\begin{equation}\label{eq:cam_gap} 
    F^{k} = \frac{1}{Z} \sum_{i} \sum_{j} A_{ij}^k
\end{equation}

CAM computes the final scores by,
\begin{equation}\label{eq:cam_scores}
    Y^c = \sum_{k} w_{k}^c \cdot F^{k}
\end{equation}
where $w_{k}^c$ is the weight connecting the $k^{th}$ feature map with the $c^{th}$ class. 

Taking the gradient of the score for class c ($Y^c$)  with respect to the feature map $F^k$ we get,
\begin{equation}\label{eq:cam_grad}
    \qquad \text{(From Chain Rule)}\\
    \frac{\partial Y^c}{\partial F^k} = \frac{\frac{\partial Y^c}{\partial A_{ij}^k}}{\frac{\partial F^k}{\partial A_{ij}^k}}
\end{equation}

Taking partial derivative of  \eqref{eq:cam_gap} \wrt $A_{ij}^k$, we can see that $\frac{\del F^k}{\del A^k_{ij}} = \frac{1}{Z}$. Substituting this in \eqref{eq:cam_grad}, we get,

\begin{equation}
    \frac{\partial Y^c}{\partial F^k} = \frac{\partial Y^c}{\partial A_{ij}^k} \cdot Z\\
\end{equation}

From \eqref{eq:cam_scores} we get that, $ \frac{\partial Y^c}{\partial F^k} = w_{k}^c$. Hence,

\begin{equation}\label{eq:cam_weights}
    w_{k}^c = Z \cdot \frac{\partial Y^c}{\partial A_{ij}^k}\\
\end{equation}

Now, we can sum both sides of this expression in (5) over all pixels $(i,j)$ to get: 

\begin{align}
    \sum_i \sum_j w^c_k  = \sum_i \sum_j Z \cdot \frac{\del Y^c}{\del A^k_{ij} }, \quad \text{which can be rewritten as} \\ 
    Z w^c_k  = Z \sum_i \sum_j \frac{\del Y^c}{\del A^k_{ij} } \quad \text{(Since $Z$ and $w^c_k$ do not depend on $(i,j)$)}
\end{align}

Note that $Z$ is the number of pixels in the feature map (or $Z = \sum_i \sum_j 1 $). 
Thus, we can re-order terms and see that:

\begin{align}
    w^c_k  &= \sum_i \sum_j \frac{\del Y^c}{\del A^k_{ij} }\\
\end{align}

We can see that up to a proportionality constant ($1/Z$) that is normalized out during visualization, the expression for $w^c_k$ is identical to $\alpha^c_k$ used by \gcam{} (as described in the main paper). 


Thus \gcam{} is a generalization of CAM to arbitrary CNN-based architectures, while maintaining the computational efficiency of CAM.

