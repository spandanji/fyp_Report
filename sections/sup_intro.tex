\section{Introduction}

In \secref{sec:sup_generalization}, we derive that \gcam{} is a generalization of CAM to \emph{any} CNN-based architecture, and hence doesn't require any architectural change or retraining. 
In \secref{sec:sup_experiments} we provide more visual explanations for image classification (VGG-16), image captioning (Neuraltalk2), and Visual Question Answering (VQA). For image captioning and VQA, our visualizations (\gcam{}, and \cgb{}) expose the somewhat surprising insight that common CNN+LSTM
models can often be good at localizing discriminative input image regions despite note being trained on grounded image-text pairs.
In \secref{sec:sup_ablation}, we explore and validate our design choices for computing \gcam{} visualizations. 
In \secref{sec:sup_localization}, we compare the localization ability of \gcam{} with previous approaches in the context of image classification.
In \secref{sec:sup_segmentation}, we show qualitative results by using weak-localization cues from \gcam{} as a seed for SEC~\cite{seed_eccv16}.
In \secref{sec:sup_comparison}, we provide qualitative examples comparing \gcam{} with CAM and c-MWP on two image classification datasets, PASCAL and COCO. We find that our visualizations are superior, while being faster to compute and while being possible to visualize a  \emph{wide variety of CNN-based models, including but not limited to, CNNs with fully-connected layers, CNNs stacked with Recurrent Neural Networks (RNNs), ResNets \etc}.


