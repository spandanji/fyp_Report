\vspace{-10pt}
\section{Conclusion}
\ad{
    In this work, we proposed a novel class-discriminative localization technique --
    Gradient-weighted Class Activation Mapping (\gcam{}) -- for making \emph{any}
    CNN-based model more transparent by producing visual explanations.
    Further, we combined \gcam{} localizations with existing high-resolution visualization
    techniques to obtain the best of both worlds -- high-resolution and class-discriminative
    \cgb{} visualizations.
}
Our visualizations outperform existing approaches on both axes -- interpretability
and faithfulness to original model. Extensive human studies reveal that our
visualizations can discriminate between classes more accurately, better expose
the trustworthiness of a classifier, and help identify biases in datasets.
Further, we devise a way to identify important neurons through Grad-CAM and provide a way to obtain textual explanations for model decisions.
Finally, we show the broad applicability of \gcam{} to various off-the-shelf
architectures for tasks such as image classification, image captioning and
visual question answering. 
We believe that a true AI system should not only be intelligent, but also be able
to reason about its beliefs and actions for humans to trust and use it.
Future work includes explaining decisions made by deep networks in domains such
as reinforcement learning, natural language processing and video applications.



\vspace{-10pt}
\section{Acknowledgements}

This work was funded in part by NSF CAREER awards to
DB and DP, DARPA XAI grant to DB and DP, ONR YIP awards to DP and DB, ONR Grant
N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO
YIP awards to DB and DP, an Allen Distinguished Investigator
award to DP from the Paul G. Allen Family Foundation,
ICTAS Junior Faculty awards to DB and DP, Google
Faculty Research Awards to DP and DB, Amazon Academic
Research Awards to DP and DB, AWS in Education
Research grant to DB, and NVIDIA GPU donations to DB. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or
implied, of the U.S. Government, or any sponsor.
